{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597410947032",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on your Facebook data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting your facebook data\n",
    "\n",
    "Downloading your facbook data is actually pretty simple! From the FaceBook website go to Settings -> Your FaceBook Information -> Download your information. From here you may tailor how much data you would like to download. Note typically these files are very large in size and that FaceBook may take a few days to give you the data.\n",
    "\n",
    "For this project all that is required is your **messages** data, downloaded as a **high** quality **JSON** file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.make_dataset import data, extract\n",
    "from src.models import predict_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter location of raw data:\n",
    "raw_data_path = 'E:\\messages\\inbox\\GurbirSinghJohal_vLCZzZ04BQ'\n",
    "\n",
    "# Extracting data\n",
    "extract(raw_data_path, output_filepath = '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 68,  38,   5, ..., 137,  81,  39])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Creating Data class\n",
    "gurb_data = data()\n",
    "\n",
    "# Enter message file loction\n",
    "gurb_data.encode(input_filepath = '..\\data\\GurbirSinghJohal_vLCZzZ04BQ\\Gurbir_Singh_Johal_messages.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model\n",
    "\n",
    "We now instigate a character-level RNN using PyTorch's NN class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CharRNN(\n  (lstm): LSTM(166, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=166, bias=True)\n)\n"
    }
   ],
   "source": [
    "# Define parameters of the RNN:\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = train_model.CharRNN(gurb_data.chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 1/80... Step: 10... Loss: 3.4641... Val Loss: 3.4132 time: 19.4\nEpoch: 2/80... Step: 20... Loss: 3.3941... Val Loss: 3.3230 time: 20.9\nEpoch: 2/80... Step: 30... Loss: 3.3340... Val Loss: 3.3036 time: 22.5\nEpoch: 3/80... Step: 40... Loss: 3.3203... Val Loss: 3.3039 time: 24.0\nEpoch: 3/80... Step: 50... Loss: 3.3023... Val Loss: 3.3017 time: 25.6\nEpoch: 4/80... Step: 60... Loss: 3.2712... Val Loss: 3.3008 time: 27.1\nEpoch: 5/80... Step: 70... Loss: 3.3122... Val Loss: 3.2988 time: 28.6\nEpoch: 5/80... Step: 80... Loss: 3.3048... Val Loss: 3.2963 time: 30.1\nEpoch: 6/80... Step: 90... Loss: 3.2880... Val Loss: 3.2952 time: 31.6\nEpoch: 6/80... Step: 100... Loss: 3.2724... Val Loss: 3.2916 time: 33.1\nEpoch: 7/80... Step: 110... Loss: 3.2582... Val Loss: 3.2874 time: 34.6\nEpoch: 8/80... Step: 120... Loss: 3.2890... Val Loss: 3.2795 time: 36.1\nEpoch: 8/80... Step: 130... Loss: 3.2556... Val Loss: 3.2679 time: 37.6\nEpoch: 9/80... Step: 140... Loss: 3.2427... Val Loss: 3.2501 time: 39.1\nEpoch: 9/80... Step: 150... Loss: 3.2224... Val Loss: 3.2184 time: 40.6\nEpoch: 10/80... Step: 160... Loss: 3.1742... Val Loss: 3.1706 time: 42.1\nEpoch: 10/80... Step: 170... Loss: 3.0934... Val Loss: 3.1012 time: 43.6\nEpoch: 11/80... Step: 180... Loss: 3.0591... Val Loss: 3.0411 time: 45.1\nEpoch: 12/80... Step: 190... Loss: 3.0160... Val Loss: 2.9841 time: 46.6\nEpoch: 12/80... Step: 200... Loss: 2.9509... Val Loss: 2.9341 time: 48.1\nEpoch: 13/80... Step: 210... Loss: 2.8899... Val Loss: 2.8707 time: 49.6\nEpoch: 13/80... Step: 220... Loss: 2.8579... Val Loss: 2.8276 time: 51.0\nEpoch: 14/80... Step: 230... Loss: 2.7969... Val Loss: 2.7839 time: 52.5\nEpoch: 15/80... Step: 240... Loss: 2.7880... Val Loss: 2.7571 time: 54.0\nEpoch: 15/80... Step: 250... Loss: 2.7607... Val Loss: 2.7250 time: 55.5\nEpoch: 16/80... Step: 260... Loss: 2.7382... Val Loss: 2.7090 time: 57.0\nEpoch: 16/80... Step: 270... Loss: 2.6917... Val Loss: 2.6833 time: 58.5\nEpoch: 17/80... Step: 280... Loss: 2.6822... Val Loss: 2.6652 time: 59.9\nEpoch: 18/80... Step: 290... Loss: 2.6945... Val Loss: 2.6459 time: 61.4\nEpoch: 18/80... Step: 300... Loss: 2.6456... Val Loss: 2.6282 time: 62.9\nEpoch: 19/80... Step: 310... Loss: 2.6356... Val Loss: 2.6117 time: 64.5\nEpoch: 19/80... Step: 320... Loss: 2.6251... Val Loss: 2.5958 time: 66.0\nEpoch: 20/80... Step: 330... Loss: 2.6126... Val Loss: 2.5826 time: 67.5\nEpoch: 20/80... Step: 340... Loss: 2.5943... Val Loss: 2.5694 time: 69.0\nEpoch: 21/80... Step: 350... Loss: 2.5860... Val Loss: 2.5489 time: 70.5\nEpoch: 22/80... Step: 360... Loss: 2.5832... Val Loss: 2.5383 time: 72.0\nEpoch: 22/80... Step: 370... Loss: 2.5581... Val Loss: 2.5179 time: 73.5\nEpoch: 23/80... Step: 380... Loss: 2.5415... Val Loss: 2.5073 time: 75.0\nEpoch: 23/80... Step: 390... Loss: 2.5370... Val Loss: 2.4904 time: 76.5\nEpoch: 24/80... Step: 400... Loss: 2.4803... Val Loss: 2.4780 time: 78.0\nEpoch: 25/80... Step: 410... Loss: 2.4886... Val Loss: 2.4633 time: 79.5\nEpoch: 25/80... Step: 420... Loss: 2.4849... Val Loss: 2.4495 time: 81.0\nEpoch: 26/80... Step: 430... Loss: 2.4731... Val Loss: 2.4394 time: 82.5\nEpoch: 26/80... Step: 440... Loss: 2.4525... Val Loss: 2.4264 time: 83.9\nEpoch: 27/80... Step: 450... Loss: 2.4428... Val Loss: 2.4139 time: 85.4\nEpoch: 28/80... Step: 460... Loss: 2.4612... Val Loss: 2.4019 time: 86.9\nEpoch: 28/80... Step: 470... Loss: 2.4139... Val Loss: 2.3907 time: 88.4\nEpoch: 29/80... Step: 480... Loss: 2.4067... Val Loss: 2.3818 time: 89.9\nEpoch: 29/80... Step: 490... Loss: 2.4018... Val Loss: 2.3672 time: 91.4\nEpoch: 30/80... Step: 500... Loss: 2.4082... Val Loss: 2.3577 time: 92.9\nEpoch: 30/80... Step: 510... Loss: 2.3882... Val Loss: 2.3455 time: 94.4\nEpoch: 31/80... Step: 520... Loss: 2.3693... Val Loss: 2.3334 time: 95.9\nEpoch: 32/80... Step: 530... Loss: 2.3896... Val Loss: 2.3278 time: 97.4\nEpoch: 32/80... Step: 540... Loss: 2.3698... Val Loss: 2.3128 time: 98.9\nEpoch: 33/80... Step: 550... Loss: 2.3573... Val Loss: 2.3052 time: 100.4\nEpoch: 33/80... Step: 560... Loss: 2.3438... Val Loss: 2.2902 time: 101.9\nEpoch: 34/80... Step: 570... Loss: 2.3025... Val Loss: 2.2798 time: 103.4\nEpoch: 35/80... Step: 580... Loss: 2.2993... Val Loss: 2.2726 time: 104.9\nEpoch: 35/80... Step: 590... Loss: 2.3104... Val Loss: 2.2610 time: 106.4\nEpoch: 36/80... Step: 600... Loss: 2.2998... Val Loss: 2.2523 time: 107.9\nEpoch: 36/80... Step: 610... Loss: 2.2874... Val Loss: 2.2440 time: 109.4\nEpoch: 37/80... Step: 620... Loss: 2.2691... Val Loss: 2.2354 time: 111.0\nEpoch: 38/80... Step: 630... Loss: 2.2952... Val Loss: 2.2304 time: 112.5\nEpoch: 38/80... Step: 640... Loss: 2.2441... Val Loss: 2.2152 time: 114.0\nEpoch: 39/80... Step: 650... Loss: 2.2397... Val Loss: 2.2080 time: 115.5\nEpoch: 39/80... Step: 660... Loss: 2.2451... Val Loss: 2.2021 time: 117.0\nEpoch: 40/80... Step: 670... Loss: 2.2551... Val Loss: 2.1915 time: 118.5\nEpoch: 40/80... Step: 680... Loss: 2.2339... Val Loss: 2.1819 time: 120.0\nEpoch: 41/80... Step: 690... Loss: 2.2233... Val Loss: 2.1731 time: 121.5\nEpoch: 42/80... Step: 700... Loss: 2.2393... Val Loss: 2.1670 time: 123.0\nEpoch: 42/80... Step: 710... Loss: 2.2236... Val Loss: 2.1576 time: 124.5\nEpoch: 43/80... Step: 720... Loss: 2.2171... Val Loss: 2.1518 time: 126.0\nEpoch: 43/80... Step: 730... Loss: 2.2025... Val Loss: 2.1454 time: 127.4\nEpoch: 44/80... Step: 740... Loss: 2.1490... Val Loss: 2.1364 time: 128.9\nEpoch: 45/80... Step: 750... Loss: 2.1663... Val Loss: 2.1273 time: 130.4\nEpoch: 45/80... Step: 760... Loss: 2.1734... Val Loss: 2.1181 time: 131.9\nEpoch: 46/80... Step: 770... Loss: 2.1683... Val Loss: 2.1113 time: 133.4\nEpoch: 46/80... Step: 780... Loss: 2.1552... Val Loss: 2.1041 time: 134.9\nEpoch: 47/80... Step: 790... Loss: 2.1379... Val Loss: 2.0975 time: 136.4\nEpoch: 48/80... Step: 800... Loss: 2.1576... Val Loss: 2.0948 time: 137.9\nEpoch: 48/80... Step: 810... Loss: 2.1184... Val Loss: 2.0832 time: 139.4\nEpoch: 49/80... Step: 820... Loss: 2.1094... Val Loss: 2.0769 time: 140.9\nEpoch: 49/80... Step: 830... Loss: 2.1240... Val Loss: 2.0726 time: 142.4\nEpoch: 50/80... Step: 840... Loss: 2.1246... Val Loss: 2.0627 time: 144.0\nEpoch: 50/80... Step: 850... Loss: 2.1198... Val Loss: 2.0593 time: 145.5\nEpoch: 51/80... Step: 860... Loss: 2.1026... Val Loss: 2.0520 time: 147.1\nEpoch: 52/80... Step: 870... Loss: 2.1225... Val Loss: 2.0456 time: 148.7\nEpoch: 52/80... Step: 880... Loss: 2.0951... Val Loss: 2.0419 time: 150.2\nEpoch: 53/80... Step: 890... Loss: 2.0930... Val Loss: 2.0374 time: 151.8\nEpoch: 53/80... Step: 900... Loss: 2.0787... Val Loss: 2.0278 time: 153.4\nEpoch: 54/80... Step: 910... Loss: 2.0382... Val Loss: 2.0203 time: 154.9\nEpoch: 55/80... Step: 920... Loss: 2.0447... Val Loss: 2.0183 time: 156.5\nEpoch: 55/80... Step: 930... Loss: 2.0662... Val Loss: 2.0126 time: 158.1\nEpoch: 56/80... Step: 940... Loss: 2.0530... Val Loss: 2.0069 time: 159.6\nEpoch: 56/80... Step: 950... Loss: 2.0445... Val Loss: 1.9999 time: 161.2\nEpoch: 57/80... Step: 960... Loss: 2.0221... Val Loss: 1.9969 time: 162.8\nEpoch: 58/80... Step: 970... Loss: 2.0490... Val Loss: 1.9927 time: 164.3\nEpoch: 58/80... Step: 980... Loss: 2.0147... Val Loss: 1.9829 time: 165.9\nEpoch: 59/80... Step: 990... Loss: 2.0054... Val Loss: 1.9793 time: 167.5\nEpoch: 59/80... Step: 1000... Loss: 2.0173... Val Loss: 1.9743 time: 169.0\nEpoch: 60/80... Step: 1010... Loss: 2.0168... Val Loss: 1.9728 time: 170.6\nEpoch: 60/80... Step: 1020... Loss: 2.0015... Val Loss: 1.9668 time: 172.2\nEpoch: 61/80... Step: 1030... Loss: 1.9913... Val Loss: 1.9620 time: 173.8\nEpoch: 62/80... Step: 1040... Loss: 2.0128... Val Loss: 1.9588 time: 175.3\nEpoch: 62/80... Step: 1050... Loss: 1.9912... Val Loss: 1.9521 time: 176.8\nEpoch: 63/80... Step: 1060... Loss: 1.9980... Val Loss: 1.9456 time: 178.3\nEpoch: 63/80... Step: 1070... Loss: 1.9863... Val Loss: 1.9441 time: 179.8\nEpoch: 64/80... Step: 1080... Loss: 1.9515... Val Loss: 1.9404 time: 181.2\nEpoch: 65/80... Step: 1090... Loss: 1.9513... Val Loss: 1.9401 time: 182.7\nEpoch: 65/80... Step: 1100... Loss: 1.9731... Val Loss: 1.9342 time: 184.2\nEpoch: 66/80... Step: 1110... Loss: 1.9576... Val Loss: 1.9283 time: 185.7\nEpoch: 66/80... Step: 1120... Loss: 1.9435... Val Loss: 1.9246 time: 187.2\nEpoch: 67/80... Step: 1130... Loss: 1.9287... Val Loss: 1.9195 time: 188.7\nEpoch: 68/80... Step: 1140... Loss: 1.9583... Val Loss: 1.9158 time: 190.2\nEpoch: 68/80... Step: 1150... Loss: 1.9275... Val Loss: 1.9119 time: 191.7\nEpoch: 69/80... Step: 1160... Loss: 1.9103... Val Loss: 1.9099 time: 193.1\nEpoch: 69/80... Step: 1170... Loss: 1.9247... Val Loss: 1.9048 time: 194.6\nEpoch: 70/80... Step: 1180... Loss: 1.9246... Val Loss: 1.9002 time: 196.1\nEpoch: 70/80... Step: 1190... Loss: 1.9237... Val Loss: 1.8967 time: 197.6\nEpoch: 71/80... Step: 1200... Loss: 1.9090... Val Loss: 1.8954 time: 199.1\nEpoch: 72/80... Step: 1210... Loss: 1.9394... Val Loss: 1.8918 time: 200.6\nEpoch: 72/80... Step: 1220... Loss: 1.9091... Val Loss: 1.8886 time: 202.1\nEpoch: 73/80... Step: 1230... Loss: 1.9231... Val Loss: 1.8848 time: 203.6\nEpoch: 73/80... Step: 1240... Loss: 1.8923... Val Loss: 1.8879 time: 205.1\nEpoch: 74/80... Step: 1250... Loss: 1.8684... Val Loss: 1.8829 time: 206.6\nEpoch: 75/80... Step: 1260... Loss: 1.8752... Val Loss: 1.8769 time: 208.1\nEpoch: 75/80... Step: 1270... Loss: 1.8842... Val Loss: 1.8736 time: 209.6\nEpoch: 76/80... Step: 1280... Loss: 1.8665... Val Loss: 1.8675 time: 211.2\nEpoch: 76/80... Step: 1290... Loss: 1.8618... Val Loss: 1.8682 time: 212.7\nEpoch: 77/80... Step: 1300... Loss: 1.8541... Val Loss: 1.8679 time: 214.3\nEpoch: 78/80... Step: 1310... Loss: 1.8827... Val Loss: 1.8623 time: 215.9\nEpoch: 78/80... Step: 1320... Loss: 1.8527... Val Loss: 1.8603 time: 217.4\nEpoch: 79/80... Step: 1330... Loss: 1.8385... Val Loss: 1.8567 time: 219.0\nEpoch: 79/80... Step: 1340... Loss: 1.8541... Val Loss: 1.8537 time: 220.6\nEpoch: 80/80... Step: 1350... Loss: 1.8467... Val Loss: 1.8511 time: 222.1\nEpoch: 80/80... Step: 1360... Loss: 1.8503... Val Loss: 1.8493 time: 223.7\n"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 80 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train_model.train(net, gurb_data, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lang\nTo do it’s aloin is tho\nIm a this to some mine there\nWana go a fucking thats\nShaptit\nAlses for u\nWe seen the perg to do\nI defent to get init\nIm gna go sore anyonine\nThe plobabiry if or camar thr shit\nTrying in later long mare\nLang\nI doennt don’t we can’t dingers in toll in the more and im a chist me ban is tore take mate in my\nIdk\nThis is is the from sume stuff\nLove u did in that then then\nLol them stull is a forget an the curd\nThen we go shit it onengre init\nLol\nIndenting sandig lang\nYh hear u went it\nI do it as my room\nI did its a thower in\nWhat i weer\nWhere u said\nLol they call starts\nWat mate mate\nThey warking torry\nIn my fucking sore\nI have a few mines of there in library\nIn lol\nLang have to shit in the farm\nSak\nI have to go stop the peng\nWith up to go they they doing them\nSure me\nI shudve a bo leave it\nWas i doing to start to come but its a they day\nWalk same to hand the recker i week in a funded\nWell will see work sem up on so tord it\nThen suco u\nIt doing\nShope is it to go shade\n"
    }
   ],
   "source": [
    "print(predict_model.sample(net, 1000, prime='lang', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model:\n",
    "\n",
    "train_model.save(net, name = 'Gurbot', loc = '..\\models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model:\n",
    "\n",
    "net2 = train_model.load('..\\models\\Gurbot.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lang u didnt get it is a torda is sonter seenst me thit lol\nWhit wen is time in mad the confucting\nLmfao\nShort work u done lol\nWhat we was some that work\nIdk\nI cant see it was it or some mich is to did it the fucking stiffering them\nLinear all be library\nI cunt shit is sheets fucked\nLmao\nLol what to used a fit\nAnd sell\nIn my to do to deen there shit\nWhere u going things aswell we dont warking sormer\nShough u say at in the course sick they\nWeer to stepper the sheets a see we can me u go the piss\nWhere u go sumbred\nLot like u wont analying it then\nIn the probabit\nThe didnt mean in man\nLike im gna cand andersting in attell\nLol\nU wana get me and this it was it to sard in the prigurt\nIn literally witho u do it\nIts are u seats\nI he wud gat my time\nCome treap\nSo what we get it to tell\nYes i dont have\nNever started and shutt all dont a calm\nLool it\nWe want tayed a to me\nLang\nU did in the comporte all the from it agee is one is it\nIm gna do it\nThat\nWe can do it\nLol\nYep i seen u day went\nWe changed i\n"
    }
   ],
   "source": [
    "# Sample loaded model:\n",
    "\n",
    "print(predict_model.sample(net2, 1000, prime='lang', top_k=5))"
   ]
  }
 ]
}