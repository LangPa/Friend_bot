{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_1.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    data1 = json.load(read_file)\n",
    "\n",
    "with open(\"../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_2.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    data2 = json.load(read_file) \n",
    "\n",
    "with open(\"../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_3.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    data3 = json.load(read_file)\n",
    "\n",
    "messages = data1['messages'] + data2['messages'] + data3['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21478"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['participants', 'messages', 'title', 'is_still_participant', 'thread_type', 'thread_path'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Gurbir Singh Johal'}, {'name': 'Lang Parkin'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['participants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\messages\\inbox\\WestHouseDrugsLads_XJAnkeCdUg\\message_1.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    data5 = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/GurbirSinghJohal_vLCZzZ04BQ/message_contents.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    data6 = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lang Parkin': 'Gonna do some this week\\nStill got your super strong stuff haha\\nOh shit  got u\\nDaffied?\\nBut nah youre right\\nHaha it was dione who got the graph alphas mate\\nHavent put my mind to anything in 9 months its time to change\\nI know man, Im gonna start trying again\\nYh go on\\nHahaha i dont doubt that\\nYeah true lmao\\nYo what app do you to do your trading? Im gonna try it\\nWill deffo give that a read mate, so far my astronomy attempts have been pitiful lol\\nMeant to reply to this\\nHow have they not been studying the plane our own galaxy lies in?\\nSo its all waiting on unproven theories\\nWe went down to see the sunset but missed it lol\\nThats insane mate, can only imagine the significance of what it could mean\\nLooks mad tho, what results did you actually get from it?\\nYeah only trees are only good for looking at the decision process, forests better\\nWait give me 20 mins actually its dinner\\nCan i see it?\\nYo thats sick\\nThe black holes one?\\nWhat you working on?\\nHahaha\\n',\n",
       " 'Gurbir Singh Johal': 'Modaf\\nBut I’m daffied up to the fucjing brim\\nU might be able to tell\\nC’s\\nIf I did that I’d be working in construction or smth rn wen I was fucking getting ax’s in Yr 11 maths\\nLegit how many people just get limited cuz they think they can’t be as good as someone else\\nConfidence is massively underestimated man\\nLegit\\nI just want u to back urself fam\\nLike fam every time I say quant/traderetc ur like u can never get that \\nFam r u acc mad \\nU are fucking smart \\nTripos just smashed ur confidence, u got 3 graph alphas fam and u think ur shit r u mad \\nofcourse we fucking can fucking get into the top quant funds \\nNO ONE gets in without doing hardcore prep, I remember when Baran knew fuck all\\nU lack a lot of confidence \\nStop believing that \\nIt is fucking absolute bullshit\\nWe can do anything we put our minds too \\nbecome quants SOft Engineers traders etc\\nBro as a brother\\nThis will seriously make u doubt urself \\nCuz it’s fucking hard lol\\nU gotta back urself too fam\\nLike learn for a month \\nThen u can just copy me and learn thru tht ygm\\nThe more u rush the more money u will lose\\nNo rush\\nI’d learn first if I was u\\nYh fam\\nWyS\\ngang\\nthis explains the dataset and its consequences\\nhttps://www.ztf.caltech.edu/page/science\\nread this\\nlike legit because its so dense loool\\nbecause it so dense\\nits gna take like 20 yrs for them to fully understand the dataset\\nthis dataset they say, will change astronomy for decades lol\\nbefore this dataset\\nand its never been studeied before\\nand basically this plane is 1000 more dense then the average\\nie our galaxy lies\\nthis is for the galactic plane\\nthe good thing is\\nno not neccessaruily\\nbecause it depends on a perfect allignment which doesnt happen again sometimes lol\\nsometimes they may never observe it ever again\\nthey will have to wait ages\\nto confirk shit\\nthe hard work is still to be done, cuz now\\nyou can make rough guesses with a paramter called the einstein timescale\\nblack holes, binary black holes, supernoave etc\\nthey could be a range of things\\nno one know what they are tho\\nwhich is roughly good cuz of the probability at which we expect to find them ygm\\nmy new one has like 356 hits out of 50 million\\ngot gravitationallt microlensed events\\nbut importance is .95 in one ef em, u know the tree just remembered dumb fuking tree\\nfeature importance rlly good, cuz like u know if there 3 obvious equally important features\\nand keep squaring the number of trees u usin and look at performance cuz it starts to taper off right\\nrandom forestS the ting\\nfuking then\\ndecision tree then incresead depth obvs overfiti\\nmL\\nmost it rlly is just fucking around with the dataset lol\\nand the erro scaling\\nbare experimentation and reading papers went into those choices of the numbers\\nthis is trining st\\nwut u wna see fam\\nyhh\\nso just gna improve on that\\nand the problem is hard\\ncuz like there stil lbare data\\natm, imporoving my code from my project that i handed in\\nfucking classified the shit outta this\\ndoing some fuking machine learning yhhhh\\nmate im fucking high as fucking fuck\\nLANG PARKIN\\nLANG PARKINGS\\nLAN PAR\\nHELO LAN\\n'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Irrah Carver-Jones'},\n",
       " {'name': 'Kyung Mo Koo'},\n",
       " {'name': 'Ben van Vlymen'},\n",
       " {'name': 'Owen Underwood'},\n",
       " {'name': 'Daniel Tong'},\n",
       " {'name': 'Alex Lipov'},\n",
       " {'name': 'Pat Wichit'},\n",
       " {'name': 'Michael Harvey'},\n",
       " {'name': 'Tom Jack Adams'},\n",
       " {'name': 'Laksh Aithani'},\n",
       " {'name': 'Matthew Koster'},\n",
       " {'name': 'Matthew Adesina'},\n",
       " {'name': 'Joel Essam'},\n",
       " {'name': 'Ertug Aytug'},\n",
       " {'name': 'Qiangru Kuang'},\n",
       " {'name': 'Leo Zlotnikov'},\n",
       " {'name': 'Sam Bird'},\n",
       " {'name': 'Shane Gill Mallard'},\n",
       " {'name': 'Gurbir Singh Johal'},\n",
       " {'name': 'Lang Parkin'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5['participants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lang Parkin': 'Gonna do some this week\\nStill got your super strong stuff haha\\nOh shit  got u\\nDaffied?\\nBut nah youre right\\nHaha it was dione who got the graph alphas mate\\nHavent put my mind to anything in 9 months its time to change\\nI know man, Im gonna start trying again\\nYh go on\\nHahaha i dont doubt that\\nYeah true lmao\\nYo what app do you to do your trading? Im gonna try it\\nWill deffo give that a read mate, so far my astronomy attempts have been pitiful lol\\nMeant to reply to this\\nHow have they not been studying the plane our own galaxy lies in?\\nSo its all waiting on unproven theories\\nWe went down to see the sunset but missed it lol\\nThats insane mate, can only imagine the significance of what it could mean\\nLooks mad tho, what results did you actually get from it?\\nYeah only trees are only good for looking at the decision process, forests better\\nWait give me 20 mins actually its dinner\\nCan i see it?\\nYo thats sick\\nThe black holes one?\\nWhat you working on?\\nHahaha\\n',\n",
       " 'Gurbir Singh Johal': 'Modaf\\nBut Iâ\\x80\\x99m daffied up to the fucjing brim\\nU might be able to tell\\nCâ\\x80\\x99s\\nIf I did that Iâ\\x80\\x99d be working in construction or smth rn wen I was fucking getting axâ\\x80\\x99s in Yr 11 maths\\nLegit how many people just get limited cuz they think they canâ\\x80\\x99t be as good as someone else\\nConfidence is massively underestimated man\\nLegit\\nI just want u to back urself fam\\nLike fam every time I say quant/traderetc ur like u can never get that \\nFam r u acc mad \\nU are fucking smart \\nTripos just smashed ur confidence, u got 3 graph alphas fam and u think ur shit r u mad \\nofcourse we fucking can fucking get into the top quant funds \\nNO ONE gets in without doing hardcore prep, I remember when Baran knew fuck all\\nU lack a lot of confidence \\nStop believing that \\nIt is fucking absolute bullshit\\nWe can do anything we put our minds too \\nbecome quants SOft Engineers traders etc\\nBro as a brother\\nThis will seriously make u doubt urself \\nCuz itâ\\x80\\x99s fucking hard lol\\nU gotta back urself too fam\\nLike learn for a month \\nThen u can just copy me and learn thru tht ygm\\nThe more u rush the more money u will lose\\nNo rush\\nIâ\\x80\\x99d learn first if I was u\\nYh fam\\nWyS\\ngang\\nthis explains the dataset and its consequences\\nhttps://www.ztf.caltech.edu/page/science\\nread this\\nlike legit because its so dense loool\\nbecause it so dense\\nits gna take like 20 yrs for them to fully understand the dataset\\nthis dataset they say, will change astronomy for decades lol\\nbefore this dataset\\nand its never been studeied before\\nand basically this plane is 1000 more dense then the average\\nie our galaxy lies\\nthis is for the galactic plane\\nthe good thing is\\nno not neccessaruily\\nbecause it depends on a perfect allignment which doesnt happen again sometimes lol\\nsometimes they may never observe it ever again\\nthey will have to wait ages\\nto confirk shit\\nthe hard work is still to be done, cuz now\\nyou can make rough guesses with a paramter called the einstein timescale\\nblack holes, binary black holes, supernoave etc\\nthey could be a range of things\\nno one know what they are tho\\nwhich is roughly good cuz of the probability at which we expect to find them ygm\\nmy new one has like 356 hits out of 50 million\\ngot gravitationallt microlensed events\\nbut importance is .95 in one ef em, u know the tree just remembered dumb fuking tree\\nfeature importance rlly good, cuz like u know if there 3 obvious equally important features\\nand keep squaring the number of trees u usin and look at performance cuz it starts to taper off right\\nrandom forestS the ting\\nfuking then\\ndecision tree then incresead depth obvs overfiti\\nmL\\nmost it rlly is just fucking around with the dataset lol\\nand the erro scaling\\nbare experimentation and reading papers went into those choices of the numbers\\nthis is trining st\\nwut u wna see fam\\nyhh\\nso just gna improve on that\\nand the problem is hard\\ncuz like there stil lbare data\\natm, imporoving my code from my project that i handed in\\nfucking classified the shit outta this\\ndoing some fuking machine learning yhhhh\\nmate im fucking high as fucking fuck\\nLANG PARKIN\\nLANG PARKINGS\\nLAN PAR\\nHELO LAN\\n'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = {}\n",
    "for message in messages[:100]:\n",
    "    if 'content' in message:\n",
    "        D[message['sender_name']] = D.get(message['sender_name'], '') + message['content'] + '\\n'\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sender_name': 'Gurbir Singh Johal',\n",
       " 'timestamp_ms': 1523301025999,\n",
       " 'content': 'I did 1 question a few days ago',\n",
       " 'type': 'Generic'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Still got your super strong stuff haha'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['messages'][1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'reactions', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'audio_files', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'audio_files', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'audio_files', 'reactions', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'audio_files', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'videos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'sticker', 'type'])\n",
      "dict_keys(['sender_name', 'timestamp_ms', 'photos', 'type'])\n"
     ]
    }
   ],
   "source": [
    "gurbir_chat = ''\n",
    "lang_chat = ''\n",
    "for message in messages:\n",
    "    if message['sender_name'] == 'Gurbir Singh Johal':\n",
    "        try:\n",
    "            gurbir_chat += message['content'].encode('latin1').decode('utf8') + '\\n'\n",
    "        except:\n",
    "            print(message.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247229"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gurbir_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modaf\\nBut I’m daffied up to the fucjing brim\\nU might be able to tell\\nC’s\\nIf I did that I’d be workin'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gurbir_chat[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV\n",
      "Dont waste the valuable time\n",
      "Get drunk at night\n",
      "Do maths in the day\n",
      "Even tho i shud be doing more met top since i havent completed the first sheet yettt\n",
      "Ive been doing lin alg and grm\n",
      "Im doig bits of grm\n",
      "Hahaha same\n",
      "Mathematics\n",
      "Focus\n",
      "Lang\n",
      "Got my borhter to pick me up\n",
      "Ive gone home now anyway\n",
      "Why u blanking me for u cunt\n",
      "Parking soace\n",
      "Met top tings come come come\n",
      "Lol\n",
      "U alive\n",
      "I seen him leave\n",
      "Yeh\n",
      "Dw\n",
      "Stephen gone in taxi lol\n",
      "Thought u were gna be my met n top broski :(\n",
      "Oh kool\n",
      "Where u drinkin\n",
      "Met top\n",
      "Idk\n",
      "Where u wyd\n",
      "Lang\n",
      "Yes\n",
      "Its ok\n",
      "U will pass\n",
      "U will be fine\n",
      "Thats ok then\n",
      "I mean today\n",
      "How many alphas/betas u get?\n",
      "Dw so did I\n",
      "Idk i feel like i have got a 3rd\n",
      "We are together foreveer\n",
      "Ive got a 3rd\n",
      "Dw\n",
      "Uve passed right\n",
      "Lang\n",
      "Where u\n",
      "Im getting ready now\n",
      "Ayt\n",
      "Lol\n",
      "That shes a bitch\n",
      "Yeh it is peak\n",
      "Me and her fell out massively lmao\n",
      "What did ina say to you\n",
      "I seen u walking across from the library\n",
      "Let me get ready\n",
      "In 10 minutes\n",
      "Alright\n",
      "Lol\n",
      "Veff\n",
      "Stable unstabkeorbits etc\n",
      "Leanr the orbits secrion man\n",
      "Im actually getting a 2:2\n",
      "I lost that alpha\n",
      "I cba\n",
      "Idk ik just done man\n",
      "Kms im fuckrd\n",
      "Not bothering with tmmrw\n",
      "Im legit fucked\n",
      "One of my alphas is gone\n",
      "Lang\n",
      "Gna find us place in library\n",
      "Kk\n",
      "Im here\n",
      "Kool\n",
      "Im going library\n",
      "Might come\n",
      "Idk actually\n",
      "I will probably stay here for the meanwhile\n",
      "Im in my room atm\n",
      "wru\n",
      "Not library\n",
      "Im in study room btw\n",
      "Im going library\n",
      "Yo\n",
      "Im sitting wih laksh\n",
      "Plodge\n",
      "Come top of plidge\n",
      "Where are u\n",
      "Lol we sid hhahahahahaha\n",
      "This isnpure reality\n",
      "Im actually getting a 2:2\n",
      "I got one beta\n",
      "Everyone did the integral wrong\n",
      "Lmao lang\n",
      "Fantastic!!\n",
      "Ill be there in 15 minutes\n",
      "Yo u wana go buttery?\n",
      "We r going to get the food\n",
      "Sitting\n",
      "Go to where we weree sutting\n",
      "Lang im leaving now w\n",
      "Whete u at\n",
      "Yo\n",
      "Lmao thas why i said just go haha\n",
      "Yeh exactly\n",
      "So no point wasting time\n",
      "Gna shower\n",
      "Ill be like 30 minutes\n",
      "Actually go\n",
      "N il come with ya\n",
      "Why dont u wait\n",
      "Also who u getting brunch with\n",
      "So im not coming\n",
      "Hes gna turn up\n",
      "Stephen?\n",
      "Kms\n",
      "Where u at\n",
      "Im here\n",
      "In 3 minss\n",
      "Meet me in foyer now\n",
      "Come lets goooo\n",
      "Im bot in a great mood as it is\n",
      "I will fucking go crazy at him if he talks to me\n",
      "Cause he alwasy does that\n",
      "He will\n",
      "He will walk past and see us in there\n",
      "Hes a fucking weirdo\n",
      "He will fucking find us\n",
      "But steven\n",
      "Hmm maybe idk\n",
      "Lets go somewhere else\n",
      "Someones took ur spot\n",
      "Yeh but like\n",
      "Whrre u at G\n",
      "???\n",
      "Yo is steven still there\n",
      "Make sure u learn the gauss shite\n",
      "Lol\n",
      "Hes only learning the rocket equation?\n",
      "I meant\n",
      "Lol\n",
      "What he is?\n",
      "Ill overtake him on paper 3/4\n",
      "No worries\n",
      "Wow\n",
      "Hmm ok\n",
      "3?\n",
      "2?\n",
      "How many has he got\n",
      "I will literally kms\n",
      "If he gets more alphas then me\n",
      "I swear down\n",
      "Go cram by urself u fraud\n",
      "Guys where we cramming VC\n",
      "Cant he  just fuck off to marha or some shit\n",
      "Haha thats why i aint coming man\n",
      "Hes so fucking annoying\n",
      "Where u workin\n",
      "Paper 3 is gna be shit\n",
      "Idk\n",
      "Im gna try to sleep\n",
      "Oh yeh kelving got full alphas yesterdays\n",
      "I have like 4 alphas ,5 betas so far\n",
      "I mesn ive got the 2:i\n",
      "5 alphas 4 betas paper 4 I NEED\n",
      "My brains just on overdrive atm\n",
      "Idk\n",
      "What the fuck do i do\n",
      "5hrs,6rs,4hrs respectively\n",
      "Ive slept in the last 3 days\n",
      "I cant sleep\n",
      "Hey lang\n",
      "Yeh\n",
      "Lmao\n",
      "The seciron I de's i finished them both within 5 minutes\n",
      "Fucked up half of the second part of the other de'\n",
      "Fucked up the wronskian\n",
      "I did the first section II perfectly\n",
      "Idk i kinda fucked it up\n",
      "Study rooms\n",
      "I cam eto r room and everythinf u know\n",
      "I was so worried\n",
      "?!?\n",
      "How many alphas/betas\n",
      "Calm\n",
      "Where are u\n",
      "You missed a call from Gurbir.\n",
      "U ok\n",
      "How was it\n",
      "Where are u\n",
      "Lang\n",
      "Lol\n",
      "Bring me a water bottle PLEASE\n",
      "If not\n",
      "U lEFT ???\n",
      "LANG\n",
      "Shoes\n",
      "Puti n amsshies pn\n",
      "Ok coming\n",
      "Im coming foyer in 5 minutes\n",
      "Wait for me\n",
      "Where the fuck are u hahah\n",
      "Lang\n",
      "Come my room\n",
      "Gurbir called you.\n",
      "We walked back\n",
      "Im near dominoes bro\n",
      "Writing big\n",
      "When uudid the thing ahout thebautistic person\n",
      "The two people on the left\n",
      "They both gave u such a durty look\n",
      "I believe tbat with my heart\n",
      "I still think im naturally better then kelvin\n",
      "Yeh if he beats me ill kill myself\n",
      "You missed a call from Gurbir.\n",
      "Whatever\n",
      "Ill just cone study rooms\n",
      "Actually no\n",
      "Need to concentrate\n",
      "Im gna go to the booths in library\n",
      "Ok\n",
      "I cant come now\n",
      "Yeh ill go in 20 minutes\n",
      "Im gna fcking flop\n",
      "Literally shitting it\n",
      "Im shitting it\n",
      "R u not nervous\n",
      "Lang\n",
      "Pang\n",
      "Which one\n",
      "Lang wher ru\n",
      "gt innnnnnn sonnn\n",
      "wudda got 16\n",
      "the why or why not parts are each 2 marks lmao\n",
      "so easy\n",
      "we would've both got alphas hahah\n",
      "for de's\n",
      "2011 has a mark scheme\n",
      "At 5\n",
      "Kms lmao\n",
      "Ive got one wih wadsley tmmrw\n",
      "Whens ur supo for prob?\n",
      "I roughly think i onow whats going oj\n",
      "Ill explain it to you\n",
      "Hahaha\n",
      "Need to print off 2013\n",
      "Meet me downstairs library\n",
      "I aint walking up there\n",
      "Fuck that\n",
      "Going buttery first\n",
      "9:45 im starting\n",
      "Library\n",
      "Ok do paper\n",
      "What about brunch??\n",
      "Why arent they doing breakfast\n",
      "Paper 1\n",
      "Kms\n",
      "Im gna do 2013\n",
      "Ill meet u there\n",
      "U lot left yet?\n",
      "Lang im doing a mock at 9:30\n",
      "Mab\n",
      "Im in man\n",
      "Whereu\n",
      "Where U\n",
      "Hey\n",
      "Lang where r u\n",
      "For 2:15 ish\n",
      "Ok so me Nd emma gna come\n",
      "Robs going he just text me\n",
      "Kk\n",
      "Keep that in mind lang\n",
      "I may not come\n",
      "My heads hurtin alot kms\n",
      "Ill come later hopefully\n",
      "U go\n",
      "U woke me up\n",
      "Iwassleep kms\n",
      "Did VC 2010\n",
      "Just going sleep now\n",
      "Going cms at like 12:30 porbs\n",
      "I cant sleep\n",
      "good night\n",
      "very nice\n",
      "u alright\n",
      "9\n",
      "Im goig cms tmmrw morning aswell btw\n",
      "Is laksh the biggest twat every born\n",
      "Is it official\n",
      "Im going sleep\n",
      "Wake up U CUNT\n",
      "Lang\n",
      "Where r u lang\n",
      "Ive slept for 3hrs thats it\n",
      "Coming\n",
      "In 5 mins or smth yeh\n",
      "Yeh lets go buttery first\n",
      "Maybe 2011 or 2012 Paper 1\n",
      "I do mock\n",
      "Silent ones\n",
      "Lets go upstairs today\n",
      "Yes maybe study room\n",
      "Where are u?\n",
      "I already said this\n",
      "Im not helping sorry\n",
      "Send it here\n",
      "In my room sorry\n",
      "Lol\n",
      "Sik\n",
      "Yesterdays one\n",
      "Meet u at study room\n",
      "Im going there now\n",
      "Go\n",
      "Come foyer lets\n",
      "12:25\n",
      "Yo foyer\n",
      "He needs to understand this\n",
      "How the fuck can stephen not get this???\n",
      "Lmao bob is so funny\n",
      "Yeh bruH its calm she said its calm\n",
      "She will rpely in like 15 minutes\n",
      "Shes in a supo oh yeh\n",
      "Aint that deep\n",
      "Shud be fine honestly\n",
      "But its probably fine unless she wants to discuss other stuff\n",
      "Shes not replied yet\n",
      "Ill ask her jam\n",
      "Im going with emma at 12:15 or some shit\n",
      "Inn my room\n",
      "Chatting*\n",
      "Thats all\n",
      "It just eggs me on\n",
      "I wasnt batting shit u prick\n",
      "Hopefully ill atleast get a furst lmao\n",
      "But like\n",
      "I know im probably not gna hit it\n",
      "But this gives me incentive\n",
      "If i do get top 20 i wont take any money of laksh haha\n",
      "Above plodge\n",
      "Firstl floor\n",
      "Plodge pizza here\n",
      "Come\n",
      "yep\n",
      "Lmao\n",
      "Where r u\n",
      "OK Ma G\n",
      "Mock\n",
      "Mick\n",
      "Dont ask me anything\n",
      "Dont talk to me\n",
      "Im starting\n",
      "Ok cmin\n",
      "5 mins\n",
      "Safe meet in foyer in 10\n",
      "LANY\n",
      "Study rum\n",
      "Library or smth idk whatevever\n",
      "Lets gooOOOO\n",
      "Yo blud\n",
      "Now\n",
      "Safe in elecator not\n",
      "Letss go\n",
      "Yeppp\n",
      "Wtere u at\n",
      "Just come out of supo\n",
      "Ok\n",
      "Yeh\n",
      "Space?\n",
      "Calm\n",
      "Where u working\n",
      "Ill be back dwww\n",
      "Got forma\n",
      "Im gna be gone in an hour\n",
      "Ive had to sit in a fucking booth lmao\n",
      "And there is literally no room\n",
      "Im in library\n",
      "Dotn bother\n",
      "Calm\n",
      "Lets go\n",
      "Analysis?\n",
      "Wana do maths\n",
      "Move when i come dont move yet\n",
      "Ill be there soon lol\n",
      "Where u sitting\n",
      "❤️❤️\n",
      "Sorry\n",
      "And had to sort it\n",
      "I had some shit going on\n",
      "So sorry\n",
      "Sorry for bailing yesterday\n",
      "Lang when u going libraru\n",
      "AyT\n",
      "Ill be on time\n",
      "I mean it this time\n",
      "12:30\n",
      "Lmag hall??\n",
      "Ok\n",
      "Where u working today\n",
      "Lang\n",
      "Lmao\n",
      "Na cba\n",
      "Is ina asking\n",
      "Why\n",
      "No\n",
      "I dont have one\n",
      "I cant integrate shit\n",
      "Lang\n",
      "Ok will come in 5\n",
      "Where u\n",
      "Lang\n",
      "U wana go hall first?\n",
      "Meet me there i guess unless u wana cab it with me\n",
      "Me cabbing it\n",
      "Lango\n",
      "Bae\n",
      "LangBar\n",
      "LANGBO\n",
      "Me going at 12ish\n",
      "Yep\n",
      "I here u Lang\n",
      "U shud sleep lang\n",
      "How else hahahaha\n",
      "It came on fb ur online lmao\n",
      "Y u awake\n",
      "Lang\n",
      "And revise\n",
      "Get off ur phone\n",
      "LANG\n",
      "PANG\n",
      "Ok im in foyer\n",
      "Where u\n",
      "Na man gna go cms\n",
      "Jamm\n",
      "Jamm she\n",
      "10 mins\n",
      "Im coming in 10 mins\n",
      "Ok\n",
      "He wanted to\n",
      "Speak to matthew\n",
      "If not just u 2\n",
      "Me u and matthew\n",
      "Ok\n",
      "Dyu??\n",
      "What time u going there\n",
      "Might join U tbH\n",
      "U gna go buttery to revise??\n",
      "Calm\n",
      "What u doing\n",
      "Mr lang\n",
      "Ill he in study rooms till then\n",
      "Yeh ill come buttery at 1\n",
      "Come study rooms\n",
      "Where u\n",
      "LmAO\n",
      "Lol\n",
      "Wyd\n",
      "Lang\n",
      "Lmao\n",
      "Of the main building\n",
      "Try ur card on the holder\n",
      "Ur card dont work hahahaha\n",
      "Exactly\n",
      "Lmao\n",
      "What time u going\n",
      "And im still in bed\n",
      "Our cards dont work\n",
      "We cant get into library\n",
      "UNLUCkAy\n",
      "Oh yeh\n",
      "Study roomsss\n",
      "I say\n",
      "Where u wana revise\n",
      "Calm ting\n",
      "Going lib now\n",
      "Then going library\n",
      "About to just put my washing in\n",
      "Calm\n",
      "Library???\n",
      "Blud\n",
      "Come foyer\n",
      "Ur welcome xx\n",
      "So u have to change it lol\n",
      "I did tbat\n",
      "Dont copy them word for word\n",
      "Calm\n",
      "Nothing?\n",
      "How much u actually done?\n",
      "Lmao\n",
      "Im doing met and top atm bruh\n",
      "I think\n",
      "Study rooms good\n",
      "Too far cba\n",
      "Yeh maybe maybe\n",
      "Idk why\n",
      "I really dont wana go library\n",
      "Maybe\n",
      "Yeh\n",
      "Yeh im coming in 10 mins\n",
      "Here u go my beautiful lang\n",
      "Im getting answers off rob\n",
      "We can do it\n",
      "Start googling\n",
      "Im coming chill\n",
      "Lol\n",
      "Why u leaving at 3:30\n",
      "Why\n",
      "1pm\n",
      "No\n",
      "12:45\n",
      "Ok\n",
      "Lol\n",
      "Not really hungry tbh\n",
      "Idk\n",
      "Meet there at 1?\n",
      "Ok\n",
      "On rhs\n",
      "Rmfirst floor\n",
      "Muddle table\n",
      "My stuff is though\n",
      "Na\n",
      "Na nuthin\n",
      "Yh\n",
      "Lol\n",
      "How much u done\n",
      "Where u at blud\n",
      "My heads fucked\n",
      "Too much shitgoing on\n",
      "Idgaf\n",
      "Nope\n",
      "I handing in 3 secfion I's\n",
      "Kms\n",
      "Oh wow\n",
      "i sent them to u lol\n",
      "I'm really don't fucking care\n",
      "got a phone call\n",
      "i haven't done anything\n",
      "Fuck it then\n",
      "Just come\n",
      "Come buttery\n",
      "U ask him\n",
      "Im always the one emailing these fuckers\n",
      "Fuck u\n",
      "Lol\n",
      "Nothing\n",
      "Will be back for 9\n",
      "At kings cross\n",
      "On my way\n",
      "Hal\n",
      "So yeh lmao\n",
      "I need to go london at 4:30\n",
      "There\n",
      "30 mins ill eb here\n",
      "Give me abit of time\n",
      "Yeh\n",
      "U get ur work?\n",
      "Ah\n",
      "U has ur supo??\n",
      "Hahahahaha\n",
      "Lmao\n",
      "How r U\n",
      "Wagwan\n",
      "I didnt see u today\n",
      "Bruh\n",
      "U legend\n",
      "Lmao lang\n",
      "Matthew just wanted to talk\n",
      "U know id give u some if we got pizza hahahahaha\n",
      "Naaaa\n",
      "LMAO\n",
      "Dw ill be back\n",
      "Dw dw\n",
      "Dw will be back\n",
      "Need to sort stuff out\n",
      "Cannot\n",
      "It gives me an incentive to then do them\n",
      "I printed off 2008 Papers 1-4\n",
      "Will be back vry soon\n",
      "Just gone to my room to toilet\n",
      "Ye\n",
      "Lol probably haha\n",
      "Or smth\n",
      "I did like 3-4 questions\n",
      "Na\n",
      "Lmao\n",
      "Lol\n",
      "When we doing NS lol\n",
      "Study rooms\n",
      "Come mab\n",
      "And focus on number theory\n",
      "Get off ur phone\n",
      "Lang\n",
      "So we can all sit together\n",
      "Get ina to sit with u shes going now\n",
      "Ok im gna come\n",
      "Na this secrion II 2008\n",
      "I was thing about VM till like 2am lmao\n",
      "Im lying in bed\n",
      "Maybe\n",
      "Maybe\n",
      "Hes like im gna get fucking pissed\n",
      "Then i said what u doing now\n",
      "And gave me a hug\n",
      "Hen he was like come here\n",
      "(I said thag to him yesterday in tesco lmao)\n",
      "GrM schmidt orthogonalisation is alot of marks remember\n",
      "Rememberb\n",
      "He was like oh my\n",
      "Then i said i was doing VM mate\n",
      "I cant believ this\n",
      "What are u doing here\n",
      "He was likeoh my god gurbir\n",
      "And he was drunk out of his mind\n",
      "Non homerton pple\n",
      "With some other guys\n",
      "He was walking into kings bar\n",
      "Is it really u and grabbed me\n",
      "Is it actually u gurbir\n",
      "Oh my god\n",
      "He was like\n",
      "He went mad when he seen me\n",
      "Lmao\n",
      "He was drunk out of his mind lma\n",
      "Got closer and it was him\n",
      "I saw a guy in the dark and i thought t was hugo\n",
      "When walking out from the bar part of kings\n",
      "Then went kings and did maths there\n",
      "Watching films and just chilling\n",
      "I stayed in my room till like 6pm\n",
      "Sorry didng come today\n",
      "Might go tmoz then\n",
      "Calm calm\n",
      "Library\n",
      "U still in Lib\n",
      "Lang\n",
      "Sorry didnt mean it love u lang babe\n",
      "Roll down the library stairs and die\n",
      "Cunt\n",
      "Do tripos with like zain n robert etc\n",
      "Wana go town\n",
      "Lango\n",
      "Im gna be doing mhmber sets\n",
      "U can come if u want\n",
      "Im just chilling in my room\n",
      "I dnt wana go library tbh\n",
      "Ill meet u in library wen u back\n",
      "Fuck\n",
      "Oh yeh\n",
      "Why\n",
      "Just in my room\n",
      "Naa\n",
      "Erm\n",
      "U going?\n",
      "?\n",
      "U gone lectures\n",
      "Supo\n",
      "Oh suppose\n",
      "Whrre u at\n",
      "U slept?\n",
      "Wana go buttery in bit\n",
      "Where u\n",
      "Lang\n",
      "Nvm\n",
      "Wana play pool?\n",
      "Yo\n",
      "Come library\n",
      "Na\n",
      "Why would u do that\n",
      "Why\n",
      "Idiot\n",
      "Wher U\n",
      "LANG\n",
      "Im going down NOW\n",
      "Ok meet in wh foyer\n",
      "Come we go TO da buttery\n",
      "Come buttery\n",
      "Where u at\n",
      "Yeh ill finish it today\n",
      "PLEASE\n",
      "PLEASE\n",
      "Pool\n",
      "ComING\n",
      "Wait wait few kinutes\n",
      "Ayt\n",
      "Where u\n",
      "In library\n",
      "Sick 1\n",
      "Everyone else is being cunt\n",
      "Come buttery with me\n",
      "Lang where u\n",
      "Or ill come get it now and hand it in\n",
      "Lang hand my analysis in please\n",
      "240 yeh\n",
      "Safe im coming jam\n",
      "Where u at\n",
      "LOL\n",
      "Cba with elctures now\n",
      "I wana get it back today\n",
      "Before lectures\n",
      "Lang hand my work in yeh\n",
      "Lied down on my bed and fell sleep by accident\n",
      "To show its biunded between integral of logx and logx/2 on \n",
      "[epsilon,1]\n",
      "Like really easy cersion\n",
      "U use the comparison test easy on first part\n",
      "Well\n",
      "Boom done tings\n",
      "First integrals easy to eval\n",
      "See that they nevative off each other\n",
      "Then do 1/x sub\n",
      "first integral do integrand \n",
      "epsilon to 1\n",
      "U have to do IBP few times tjough\n",
      "Works\n",
      "Wokrs\n",
      "And it literally all work\n",
      "But with epsilon instead\n",
      "So then i did my a level shit\n",
      "all of them substituions\n",
      "Substitution\n",
      "He gave me a hint\n",
      "Zain told me how to do it\n",
      "I have figured out Q7 analysis\n",
      "OK yes\n",
      "U in library?\n",
      "Na\n",
      "Lol\n",
      "Thats it\n",
      "Then just write L=Iw\n",
      "u say a=b=c\n",
      "Lol whats hard about last bit of 9 haha??\n",
      "Which ones havent u done\n",
      "Hmm\n",
      "Wut lol\n",
      "All of them are missing??\n",
      "Cuz i thight some were missig jn the printed 1s\n",
      "My written 1s\n",
      "Na my ones\n",
      "Ill bring my solutiosn\n",
      "Sorry just needed to sort some shit out\n",
      "U still there yeh\n",
      "Im gna be back soon\n",
      "Safe On my way!\n",
      "Space?\n",
      "Where r u\n",
      "By symmetry get i=j=2/3\n",
      "So do i=j=1\n",
      "Interate case by case\n",
      "Na\n",
      "Please\n",
      "Buttery\n",
      "Lang\n",
      "Thanks dickhead\n",
      "Yes very autism yes\n",
      "And sums\n",
      "Im thinking of analysis\n",
      "Rn\n",
      "Like after food n shit\n",
      "I aint gna come till later\n",
      "Na i wudda told ya lel\n",
      "Where r u\n",
      "Y\n",
      "Doing a shit\n",
      "Concentrate\n",
      "Bro\n",
      "In 5\n",
      "Coming\n",
      "Learnt so much mad shit\n",
      "Legit\n",
      "That supo was amazing\n",
      "U in Lib\n",
      "22 minutes bro\n",
      "Ok\n",
      "LANG\n",
      "U BASTARD\n",
      "Ur sleeping\n",
      "U cUNT\n",
      "LANG????\n",
      "Sounds good\n",
      "Yeh 9\n",
      "Supos at 1\n",
      "Legit\n",
      "This sheet is just fuckig stupid\n",
      "Like\n",
      "I just got so pissed off\n",
      "Fucknthis sheet\n",
      "Im gna go soon tho so like\n",
      "Yeh bro\n",
      "Hurry\n",
      "6:30 Bruh\n",
      "Its me u pat joel emma ertug\n",
      "Sick\n",
      "U still on it lang\n",
      "ertug*\n",
      "retag*\n",
      "so far Me u retag joel and emma\n",
      "callmmmm\n",
      "might still go nandos\n",
      "cba\n",
      "cab\n",
      "i cancelled it\n",
      "he does that ll the time\n",
      "its fine\n",
      "trust me he just won't come\n",
      "idk\n",
      "Hes got grm supo\n",
      "Na roberts not coming 🙁\n",
      "U can watch em all today\n",
      "Yeh i seen all the videos\n",
      "Tonight*\n",
      "U wana watch fast furious 8 to ight with like damaris and few others\n",
      "I am all autism\n",
      "I am the autism\n",
      "I am full autism\n",
      "I have no distractions\n",
      "Actually leave it\n",
      "U little cunt gt ur ass 2 da library\n",
      "Parking\n",
      "Parking bay\n",
      "Get ur ass here\n",
      "Library\n",
      "See u tmmrw\n",
      "Ayt fair fair\n",
      "Where u at\n",
      "Ill get urs\n",
      "Na im ok\n",
      "Ayt\n",
      "We r now leaving he bhttery in a second\n",
      "Safe\n",
      "Come buttery then\n",
      "U gof all ur stiuff??\n",
      "In 5 mins\n",
      "Safe me and joel will come get u in a second\n",
      "Wana go to a study room?\n",
      "U still in library\n",
      "But yeh i am\n",
      "Idk where he is\n",
      "?\n",
      "Have u given his charger back\n",
      "Matthew was asking\n",
      "Whats yr room number\n",
      "U see\n",
      "Hahah\n",
      "I have iphone\n",
      "I dont have one haha\n",
      "Calm\n",
      "Ayt safe\n",
      "Bit bait\n",
      "Dont even mention it in messages to me\n",
      "This was a mistake on my part\n",
      "Never again\n",
      "Legit\n",
      "Fam\n",
      "Like seriously\n",
      "Not even when ur drunk\n",
      "U dont mention what i said to u ever again\n",
      "Fam\n",
      "Gurbir sent an attachment.\n",
      "https://artofproblemsolving.com/community/c344430_cambz\n",
      "U will see the analysis stuff\n",
      "on the roght as you walk in\n",
      "1st floor\n",
      "Find my atuff\n",
      "Or 10 mins\n",
      "Ill be back in like 15 mins\n",
      "My stuffs in there tjo\n",
      "In my room\n",
      "Just cam eput od hall\n",
      "Lol\n",
      "Ie cool\n",
      "That means 'calm'\n",
      "Minor\n",
      "Jus need to prove it exists\n",
      "Well diff twice g''=f easy\n",
      "Solved it\n",
      "Yo\n",
      "Come anyway\n",
      "Km FUCKING S\n",
      "No space on my table\n",
      "And sup is not\n",
      "St all integrabke\n",
      "I cant find an example of a sequence\n",
      "Q2 sorry\n",
      "Q1\n",
      "Hello\n",
      "Helli\n",
      "12 hrs offline\n",
      "Mate u dead\n",
      "Langbo\n",
      "Jk no ones gna come sit next to me so its kool\n",
      "QUICK\n",
      "Come quick BRO\n",
      "Actually there is\n",
      "No space on my table tjo\n",
      "Yhh bruh\n",
      "Wher eu at\n",
      "Riemann integrabikity criteriON IS COOL\n",
      "Its cool\n",
      "Na in doing the proofs\n",
      "I understand the notes so far so its OK\n",
      "Na im doing notes\n",
      "LOL\n",
      "LOP\n",
      "Forgot how cool maths was\n",
      "Foyer\n",
      "Come bttery\n",
      "Sureo\n",
      "Full kMS\n",
      "Kms\n",
      "Hahaha\n",
      "Na havent even looked at it lol\n",
      "U going Lib?\n",
      "Room\n",
      "In my toom\n",
      "Yeh mate\n",
      "Where u at\n",
      "Lamg lang\n",
      "Ill join u man i library\n",
      "Midday\n",
      "Yehhh im back tmmrw bro\n",
      "Thats it LOL\n",
      "I did a few DE exam questions\n",
      "Nothing\n",
      "Literally\n",
      "I havent done a single thing\n",
      "Im not even lying yeh\n",
      "Lang\n",
      "Im back tmmrw\n",
      "How u doing\n",
      "Atleast he will pass his degree now\n",
      "Lmao fcking hell\n",
      "Lol probably\n",
      "Actually*\n",
      "I think sams aftually become autistic\n",
      "The questions are pretty hard\n",
      "Groups is abit shitty tbh\n",
      "Yep\n",
      "That wud just be a waste honestly\n",
      "We cant get any lower\n",
      "DE's 3 Alphas\n",
      "VC shud be ok maybe 1/2 alphas\n",
      "Its just groups Prob that are the problem\n",
      "Analysis exam qs easy so shud be ok\n",
      "Kill VM\n",
      "Kill NS and DR\n",
      "Max out DE's\n",
      "Its calm\n",
      "Ive got my strategy though\n",
      "Yeh mate\n",
      "Im not learning it now either 😂\n",
      "Ikr\n",
      "😂😂😂\n",
      "Well im gna try\n",
      "Im gna go mad today\n",
      "Only thing ive revised is VnM ☹️\n",
      "But hwos revision going overall?\n",
      "The solutions are fucking mad\n",
      "They are insanely hard\n",
      "Just fuck em tbh\n",
      "If wadsley said uck em they dont matter\n",
      "Ur wasting ur time\n",
      "Sik 1\n",
      "Stick to it man\n",
      "Na dont man\n",
      "Lmao hahahahha\n",
      "Lol i havent even done a singke question\n",
      "Idk it shud be something like that\n",
      "T_iiiiiiiii n times\n",
      "So say tensor is t n dimensions\n",
      "Of the suffix\n",
      "Then it just comes out\n",
      "And make it all equal\n",
      "Get the bottom part of the tensor\n",
      "Isnt it like\n",
      "Loooollllll\n",
      "But u have to get it\n",
      "They have ur charger\n",
      "Kll\n",
      "Fucking no one here\n",
      "Yeh ikr fuck sake\n",
      "Na my bags gone\n",
      "Fucj sake\n",
      "Put\n",
      "Where did they out ur bag\n",
      "Yo\n",
      "Im on 2nd fllor\n",
      "Coming\n",
      "Ayt\n",
      "Mate lets go\n",
      "Im coming to u\n",
      "Lets go\n",
      "They r coming now\n",
      "Idm its a film groups not needed\n",
      "Ill go butnits probs just na be me and u haha\n",
      "U still wana go?\n",
      "The cunts\n",
      "Lol they bailed\n",
      "Good ask km\n",
      "And get da shizzle\n",
      "We leave homerton at 2:25 go shop get snacks\n",
      "Movie at 2:45 ayt\n",
      "U still on it lang\n",
      "Ill come now\n",
      "Calm\n",
      "U gna be there for a while?\n",
      "Alright ill come soon\n",
      "When u in libraru\n",
      "Tell me\n",
      "I havent started it yet\n",
      "Pick up my jogging bottoms lol\n",
      "See rob\n",
      "May go town first\n",
      "Most likely im just gna come tonight\n",
      "Ayt i may come\n",
      "U in the library\n",
      "Coming back cam now\n",
      "Cheers 👍🏻\n",
      "Sorry\n",
      "Gna come collect my stuff later on in the week\n",
      "My mums ill im staying at home now\n",
      "Fam im not coming back\n",
      "See that groups revision right there\n",
      "That was on odd permutation\n",
      "Ttyl\n",
      "Ttly\n",
      "Safe\n",
      "We r gna kill it 🤜\n",
      "We need to know them back of our hand\n",
      "Learn all the NS proofs\n",
      "Calm\n",
      "Safety\n",
      "Paper 4\n",
      "The NS and Dynorel ones\n",
      "2005/2006\n",
      "Ill do the second half\n",
      "As a recap anyway\n",
      "Ive covered the first half of NS\n",
      "And test each other on proofs\n",
      "And we will start past papers\n",
      "Ill be back monday latest\n",
      "Ok so\n",
      "Parking\n",
      "Hes fucked\n",
      "Just seen stephen\n",
      "Lmao\n",
      "Yeh i dont trust u sory\n",
      "I get the right answer so fuck off\n",
      "Fucking steven\n",
      "Who said its wrog\n",
      "No its fucking not\n",
      "Will do\n",
      "In ma room bro I need to shower ive only done one question but i think i understand it\n",
      "Fuck\n",
      "Come butery wirh me\n",
      "Where r y\n",
      "Safe im coning too\n",
      "U wana eat in hall\n",
      "Lmao hes so fucked\n",
      "R u alive\n",
      "Parking\n",
      "Larking\n",
      "Im going sss now\n",
      "Ok\n",
      "All ym stuffs ij library\n",
      "Where r u\n",
      "Im in westhouse\n",
      "Thats sideways\n",
      "Are u down\n",
      "U are up\n",
      "Ive only done like a few questions\n",
      "NOW\n",
      "Not 30 mINUTES\n",
      "Now\n",
      "Come library\n",
      "Have u not said anything\n",
      "How u letting them do that\n",
      "Im already there\n",
      "Is that true\n",
      "Everyone said hey r massive druggis\n",
      "BRING THEM BACK\n",
      "0 anal questions for lang today\n",
      "U bastard\n",
      "Wtf\n",
      "WHERE\n",
      "Where are they\n",
      "I want to meet the parkins\n",
      "Livrafy\n",
      "Parking space\n",
      "Lang come formal tuesday\n",
      "So we dne\n",
      "1\n",
      "We now know loge=\n",
      "Then by def\n",
      "e^z=exp(zloge)\n",
      "Well we know\n",
      "Define e as lim (1+1/n)^n\n",
      "For last part\n",
      "Had to go catch train\n",
      "Ina\n",
      "Ask Ino\n",
      "Where iwas sitting\n",
      "I left the idea for q1 on the table bro\n",
      "Yes\n",
      "We will do the stuff fine dw\n",
      "This whole weekend\n",
      "I will help u dw mate\n",
      "Trust me lang\n",
      "Ill be back\n",
      "I need to eat bro\n",
      "LOL\n",
      "DO NOT WORRY\n",
      "I AM COMING\n",
      "fuk it\n",
      "U thought u were trapped innur dream but u were just staring at ur ceiling\n",
      "Seriously\n",
      "What\n",
      "Lowercase L or uppercase\n",
      "Or ur gmail\n",
      "Give me hotmail\n",
      "Give me ur email\n",
      "Ill pay u back for the printing when i return\n",
      "Print it off use it etc and hand it in monday morning if possible\n",
      "Plus im gna scan my work and send it via email on sunday\n",
      "Cheers for this, nice one.\n",
      "Alright, minor\n",
      "Yeh ok\n",
      "I want to tell u why cause its such a legit reason to go.\n",
      "Chill out\n",
      "Over some bullshit rumour\n",
      "Next minute ill be fucking arrested\n",
      "Rumours get out fucking quick\n",
      "Yeh r u lot stupid or something\n",
      "Im bot going for that hahah\n",
      "Chill out\n",
      "Lol\n",
      "Im 1st floor\n",
      "Lang bro\n",
      "Safe ill be in lib in 30 minutes\n",
      "Lol\n",
      "Let me buy 1 off u\n",
      "U gkt caffeine tablets\n",
      "Lmao im still stuck on Q1\n",
      "Ayt\n",
      "U in college\n",
      "Lol\n",
      "Nvm nvm\n",
      "Is robert jewish\n",
      "Oi lan\n",
      "U fucking savage\n",
      "LOLL\n",
      "Yeh safe\n",
      "I get asked like every hour lmao\n",
      "Lmao i dnt drink i dnt see the point\n",
      "Samira le grand\n",
      "They call her the big\n",
      "Shes from belgium or something\n",
      "No she aint related to me\n",
      "Lol\n",
      "BT?\n",
      "Shes proper crazy mate dnt talk to her\n",
      "She probably believes it\n",
      "Dont listen to what samira tells u \n",
      "She is batshit crazy mate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gurbir_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '£',\n",
       " 'é',\n",
       " '\\u200d',\n",
       " '‘',\n",
       " '’',\n",
       " '⃣',\n",
       " '☔',\n",
       " '☝',\n",
       " '☹',\n",
       " '☺',\n",
       " '♀',\n",
       " '♂',\n",
       " '⚡',\n",
       " '❤',\n",
       " '️',\n",
       " '🆘',\n",
       " '🇦',\n",
       " '🇺',\n",
       " '🌝',\n",
       " '🍆',\n",
       " '🏻',\n",
       " '🏼',\n",
       " '🏽',\n",
       " '🏾',\n",
       " '🏿',\n",
       " '🐝',\n",
       " '👌',\n",
       " '👍',\n",
       " '👎',\n",
       " '💁',\n",
       " '💥',\n",
       " '💧',\n",
       " '💳',\n",
       " '📅',\n",
       " '🔥',\n",
       " '😂',\n",
       " '😆',\n",
       " '😌',\n",
       " '😍',\n",
       " '😏',\n",
       " '😐',\n",
       " '😕',\n",
       " '😘',\n",
       " '😝',\n",
       " '😞',\n",
       " '😡',\n",
       " '😢',\n",
       " '😤',\n",
       " '😥',\n",
       " '😪',\n",
       " '😭',\n",
       " '🙁',\n",
       " '🙂',\n",
       " '🙃',\n",
       " '🙄',\n",
       " '🙅',\n",
       " '🚨',\n",
       " '🚴',\n",
       " '🚸',\n",
       " '🤐',\n",
       " '🤔',\n",
       " '🤙',\n",
       " '🤜',\n",
       " '🤟',\n",
       " '🤣',\n",
       " '🤥',\n",
       " '🤩',\n",
       " '🤪',\n",
       " '🤭',\n",
       " '🥇',\n",
       " '🥜',\n",
       " '🥰',\n",
       " '🦆',\n",
       " '\\U0001f9c8'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(gurbir_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message_1.json\n",
      "message_1.json\n",
      "message_2.json\n",
      "message_2.json\n",
      "message_3.json\n",
      "message_3.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('../data/raw/Gurbir'):\n",
    "    print(file)\n",
    "    print(os.fsdecode(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lang\\\\Documents\\\\Phython misc\\\\Friend_bot\\\\notebooks'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio',\n",
       " 'files',\n",
       " 'gifs',\n",
       " 'message_1.json',\n",
       " 'message_2.json',\n",
       " 'message_3.json',\n",
       " 'photos',\n",
       " 'videos']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/raw/GurbirSinghJohal_vLCZzZ04BQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_1.json'), WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_2.json'), WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_3.json')]\n",
      "..\\data\\raw\\GurbirSinghJohal_vLCZzZ04BQ\\message_1.json\n",
      "..\\data\\raw\\GurbirSinghJohal_vLCZzZ04BQ\\message_2.json\n",
      "..\\data\\raw\\GurbirSinghJohal_vLCZzZ04BQ\\message_3.json\n",
      "[WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_1.json'), WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_2.json'), WindowsPath('../data/raw/GurbirSinghJohal_vLCZzZ04BQ/message_3.json')]\n"
     ]
    }
   ],
   "source": [
    "pathlist = Path('../data/raw/GurbirSinghJohal_vLCZzZ04BQ').glob('message_*.JSON')\n",
    "print([path for path in Path('../data/raw/GurbirSinghJohal_vLCZzZ04BQ').glob('message_*.JSON')])\n",
    "for path in pathlist:\n",
    "     # because path is object not string\n",
    "     path_in_str = str(path)\n",
    "     print(path_in_str)\n",
    "print([path for path in Path('../data/raw/GurbirSinghJohal_vLCZzZ04BQ').glob('message_*.JSON')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.make_dataset import data\n",
    "from src.models import predict_model, train_model\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gurb_data = data(input_filepath = '../data/raw/GurbirSinghJohal_vLCZzZ04BQ', output_filepath = '../data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 54, 157,  12, ..., 143, 155,  22])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "gurb_data.encode(input_filepath = '..\\data\\processed\\GurbirSinghJohal_vLCZzZ04BQ\\Gurbir_Singh_Johal_messages.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "38\n"
    }
   ],
   "source": [
    "count = 0\n",
    "for x, y in gurb_data.get_batches(batch_size = 128, seq_length = 50, train_val = None):\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CharRNN(\n  (lstm): LSTM(166, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=166, bias=True)\n)\n"
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = train_model.CharRNN(gurb_data.chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training 0 1700\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 1/5... Step: 10... Loss: 3.4453... Val Loss: 3.3699 time: 140.0\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntraining 0 1700\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 2/5... Step: 20... Loss: 3.3886... Val Loss: 3.3169 time: 278.2\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 2/5... Step: 30... Loss: 3.3344... Val Loss: 3.3034 time: 384.7\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntraining 0 1700\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 3/5... Step: 40... Loss: 3.3159... Val Loss: 3.3038 time: 549.2\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 3/5... Step: 50... Loss: 3.3010... Val Loss: 3.3011 time: 679.3\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntraining 0 1700\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 4/5... Step: 60... Loss: 3.2718... Val Loss: 3.2993 time: 898.7\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntraining 0 1700\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 5/5... Step: 70... Loss: 3.3121... Val Loss: 3.2969 time: 1144.5\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nvalidating 1700 1900\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\nEpoch: 5/5... Step: 80... Loss: 3.3003... Val Loss: 3.2935 time: 1314.3\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\ntarget shape: torch.Size([128, 100]), bch*seq: 12800\n"
    }
   ],
   "source": [
    "reload(train_model)\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train_model.train(net, gurb_data, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "langtoa e\ne e  taat e at e\n  t\n  t\net a tt  ottte\n\ne   oe   t\n a e\n ae\n e      oa ae\ntt\n teane\n oea t e\nttaa et oe\nt oaan et ottte\nate  t aa a    et    o t aa e e t  e   e\n e  oeaeee\na a n  ta tat e e\nantt  oa  an e\n t\ne  tt t  e    a a    ee   et   aaa eae\ntaaea ae e  o ee  oaeeee  ete  t ttt t eet e e nt  e   e eate   a e e  teataa  ott aant aaa  et e eantt e\na eet eeeeate ne e  ae eet eae e a  a a taetaea  e  e et   e\nt\n    aeee an   aa   oan\nee  ettaat\n  ea e ta   te a      a  tta    aata at  t tt  tatet  e e  e e eate at     e\naee n\n    aaa e  tte aeate aa  o  e t   a te n a  t  a t  tteee eee n  eee\nt ete n\naane\n ae\n t\net\n\nt e ete ae n eea  e\n   et   eee e\n ee    an a    ta  te tt\n ta  t taatanate\n\n    a  tte  e ee\n t   e   e\n eeteee   att ata tanae  e   oett tee  aa  e\n t\nteete a  et  t  aeee t e a   o te    on\neettean  te eee\n   oe ta e\na  ett ett te a t\n at tet\n  onatett eaaaea n oa  et te\net  e\net\nane e   oeae  e   oeeae t\n a e\ne  anta eateat\nana ee  t  ae\nt e t  aanat ee n etaaee\n"
    }
   ],
   "source": [
    "reload(predict_model)\n",
    "print(predict_model.sample(net, 1000, prime='lang', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-d8f2f38b2531>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-d8f2f38b2531>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    time = lambda int(x): x + 0.3\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "time = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}